{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "universal sentence encoder_similarity.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avinashnanda/CategoricalEncodingBenchmark/blob/master/natural%20language%20processing/universal_sentence_encoder_similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHuGCdg5ASMJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#DAn model\n",
        "#embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/2\")\n",
        "#transformer large model\n",
        "#embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-large/3\")\n",
        "#transformer lite model\n",
        "#module = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-lite/2\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0jDyrGIK_C6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "f5964cb3-688b-44ef-e873-053949fdaeb0"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from contractions import CONTRACTION_MAP\n",
        "import re\n",
        "nltk.download('stopwords')\n",
        "stopword_list = nltk.corpus.stopwords.words('english')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "stopword_list.remove('no')\n",
        "stopword_list.remove('not')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "import unicodedata\n",
        "from tqdm import tqdm\n",
        "import json"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aF76QwH_BCah",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RycohCxzBCX3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embed = hub.Module(module_url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKKLFc0NLVLR",
        "colab_type": "text"
      },
      "source": [
        "### **Feature Extractor**\n",
        "This is just a simple function to wrap tensorflow call."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-vzeGJpBCVW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_features(texts):\n",
        "    if type(texts) is str:\n",
        "        texts = [texts]\n",
        "    with tf.Session() as sess:\n",
        "        sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "        return sess.run(embed(texts))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pJsoQog1AmSc",
        "colab": {}
      },
      "source": [
        "def remove_accented_chars(text):\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return text\n",
        "def remove_urls(text):\n",
        "    url_pattern = re.compile(r'https?://\\S+')\n",
        "    return url_pattern.sub(r'', text)\n",
        "# def remove_html(text):\n",
        "#     return BeautifulSoup(text, \"lxml\").text\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aoO4-Ne1AmSe",
        "colab": {}
      },
      "source": [
        "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
        "    \n",
        "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
        "                                      flags=re.IGNORECASE|re.DOTALL)\n",
        "    def expand_match(contraction):\n",
        "        match = contraction.group(0)\n",
        "        first_char = match[0]\n",
        "        expanded_contraction = contraction_mapping.get(match)\\\n",
        "                                if contraction_mapping.get(match)\\\n",
        "                                else contraction_mapping.get(match.lower())                       \n",
        "        expanded_contraction = first_char+expanded_contraction[1:]\n",
        "        return expanded_contraction\n",
        "        \n",
        "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
        "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
        "    return expanded_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q1cD9zUjAmSg",
        "colab": {}
      },
      "source": [
        "def remove_special_characters(text, remove_digits=False):\n",
        "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K7ET7vuVAmSj",
        "outputId": "a5b7dd26-8a4b-4c79-c843-e7640d12d53e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def simple_stemmer(text):\n",
        "    ps = nltk.porter.PorterStemmer()\n",
        "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
        "    return text\n",
        "\n",
        "simple_stemmer(\"My system keeps crashing his crashed yesterday, ours crashes daily\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'My system keep crash hi crash yesterday, our crash daili'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OY0Aj2dLAmSm",
        "outputId": "7a47780a-5f19-4d34-f7a1-3911a929637f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "tokenizer = ToktokTokenizer()\n",
        "def remove_stopwords(text, is_lower_case=False):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)    \n",
        "    return filtered_text\n",
        "\n",
        "remove_stopwords(\"The, and, if are stopwords, computer is not\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "', , stopwords , computer not'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1ItG4EPOAmSo",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "# wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
        "def lemmatize_text(text):\n",
        "    pos_tagged_text = nltk.pos_tag(text.split())\n",
        "    return \" \".join([lemmatizer.lemmatize(word) for word, pos in pos_tagged_text])\n",
        "    #return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s6jdS71EAmSq",
        "colab": {}
      },
      "source": [
        "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,url_remove =True,\n",
        "                     accented_char_removal=True, text_lower_case=True, \n",
        "                     text_lemmatization=True, special_char_removal=True, \n",
        "                     stopword_removal=False, remove_digits=True,tokenize=True):\n",
        "    \n",
        "    normalized_corpus = []\n",
        "    tokenized_texts = []\n",
        "    # normalize each document in the corpus\n",
        "    for doc in tqdm(corpus):\n",
        "        # strip HTML\n",
        "        if url_remove:\n",
        "            doc = remove_urls(doc)\n",
        "#         if html_stripping:\n",
        "#             doc = remove_html(doc)\n",
        "        # remove accented characters\n",
        "        if accented_char_removal:\n",
        "            doc = remove_accented_chars(doc)\n",
        "        # expand contractions    \n",
        "        if contraction_expansion:\n",
        "            doc = expand_contractions(doc)\n",
        "        # lowercase the text    \n",
        "        if text_lower_case:\n",
        "            doc = doc.lower()\n",
        "        # remove extra newlines\n",
        "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
        "        # lemmatize text\n",
        "        if text_lemmatization:\n",
        "            doc = lemmatize_text(doc)\n",
        "        # remove special characters and\\or digits    \n",
        "        if special_char_removal:\n",
        "            # insert spaces between special characters to isolate them    \n",
        "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
        "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
        "            doc = remove_special_characters(doc, remove_digits=remove_digits)  \n",
        "        # remove extra whitespace\n",
        "        doc = re.sub(' +', ' ', doc)\n",
        "        # remove stopwords\n",
        "        if stopword_removal:\n",
        "            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
        "            \n",
        "        normalized_corpus.append(doc)\n",
        "        \n",
        "        if tokenize:\n",
        "             tokens = tokenizer.tokenize(doc)\n",
        "        tokenized_texts.append(tokens)\n",
        "        \n",
        "    return normalized_corpus,tokenized_texts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uU9lI6DlBCQM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0dde305a-e26e-4e4e-e0d6-fb7f4ff232d5"
      },
      "source": [
        "a,b = normalize_corpus([\"Hello! avinash Who are you?\",\"what are you doing tonight\"])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:02<00:00,  1.01s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuYRUS_LN46N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = [\"who are you?\",\n",
        "        \"you are a bastard,asshole and bitch\",\n",
        "    \"where do you live?\",\n",
        "    \"what are your goals?\",\n",
        "    \"what does life mean to you?\",\n",
        "    \"\"\"I bet you know about the classic Mario game and probably have played it. You are on a quest, moving from levels to levels in search of a princess that has been kidnapped. You crush all the spooky dogs and flying turtles with the superpowers such as transforming into giant, shooting out bullets.\n",
        "But hey! On top of the energy of a naive Mario, you seamlessly have played the never-ending game of Pacman — yes the world where these elusive ghosts chase you (the ultimate player) for the reason that seems mysterious.\n",
        "Lust\n",
        "It’s funny how you move around the strange two-dimensional world, just for the sake of eating, eating and eating. Yes, the only reason you move is with this mysterious force to devour everything in the path. You can never get enough of it. In your life, there is always something more you want. That need never gets quenched. The game is a reflection of lust.\n",
        "Eternal Dissatisfaction\n",
        "Your lust never dies. You move from level to level. Your existence has no meaning. Your life is an endless rush into oblivion. You don’t realize what you have been missing the whole time. You don’t even dare to stop. Because if you do, the ghosts in your mind devour you completely. There’s always this dissatisfaction in your life that interrupts the harmony between your soul and the world outside.\n",
        "Desire\n",
        "Desire is something that bounds life itself to the existence. Your desire for more; more never ends. Money, love, life, and all the chaos. That’s all you think of. You can’t put aside your desires and be yourself. You can’t really focus on your own “I, Me and Myself” against the world that wants more and more.\n",
        "Levels upon levels upon levels, your existence thrive upon these uncanny behaviors.\n",
        "Achievement\n",
        "While life is chasing you and you are chasing the unknowns, there comes a point in life where you feel achieved. You feel you can conquer the world. You can triumph over the ghosts in your mind. That satisfaction of achievement ultimately boosts your inner pride; hence your inner peace is restored. It’s this factor that really factors out all the negativity in your life.\n",
        "But, again life happens. The achievement; you can’t really get enough of it. Hence, the ultimate cycle continues:\n",
        "Lust, dissatisfaction, desire and achievement makes you who you should be. i live in the land of nowhere. \n",
        "I am doing fine in my life which is very chaos. I am trying to find myself among chaos and peace.\"\"\",\n",
        "    \"\"\"I am paradox and there exists a theoretical limit to human mind that any imagination is not unbound yet there is something that meets the eye.\n",
        "The eye itself is an absolute organ to wisdom. Wisdom is something that is derived from mutation of knowledge with experience. Yet, there exists a paradox on how much wise a man is even if there is no bound to the lies s/he tells.\n",
        "You suffer; you struggle.\n",
        "Struggle is the path to the wisdom and yet there lies a paradox on how much one can tolerate. Tolerance is eternal; knowledge is bounded. Endurance is a paradox. Paradox in a sense that you never know how much a mind can endure until you have exposed everything to the mind.\n",
        "You can laugh; you can cry.\n",
        "It’s ok to cry. Cry you heart out. Crying is a spiritual path to let your sufferings out to the ever paradoxical world. You can laugh your ass off but you can never put your heart into the laughing unless you know what the context is.\n",
        "Every animal struggles, endures, cries and laughs.\n",
        "One can suffer spiritually as well as physically. If spiritually one feels the sufferings, there’ll be no peace of mind. Yet there lies a paradox in life.\n",
        "Paradox in life is paradoxical.\n",
        "“Paradox” itself is a paradox. You cannot define the absolute lifespan of a paradox because you never get to live physically throughout your life.\n",
        "Indeed, life mocks everyone. It’s like that little piece of paper that slits your throat and you’ll never know how you died because you were blind the whole time. You may not feel the pain. But the suffering is real. Now, knowledge is that paper and you are the wisdom.\n",
        "Knowlege pours life into nullness, yet it is can kill one’s spirit. Knowlege has never been an absolute wisdom. Absoluteness of a wisdom can never really exists. Existence in a sense one can never feel the real happiness until s/he has transcended to some meta reality.\n",
        "Reality mocks everyone. It has been from the ancient times. Time is a fuzzy feeling. Fuzziness persists. Yet, you always tell “how much” time has gone by. Because you feel time. You never really kiss it.\n",
        "Those sarcastic tone that every time gives. Time creeps yet it slips away. You never really know what it can give you.\n",
        "Time is pretty paradoxical. Paradoxical in a sense that you always procrastinate. Procrastination is real, so is your mind. Theoretically, time can fall and you can too. You can never really know the feeling.\n",
        "Now, there lies a paradox about mind itself. Mind is nothing but your brain and your brain is only the combinations of indidvidual cells.\n",
        "Now the greatest paradox of mind is the consciousness. You feel, suffer, look, taste, struggle, endure. It’s all in your mind and you still have a feeling of unity of your mind and body.\n",
        "So, who gets the wisdom? Consciousness is just a combination of chemicals and the signals that overpower your brain. There really exists a dilemma on who/what controls you and perhaps who are you in a true sense.\n",
        "In the end, you are just living a life full of paradoxes and whole reality you perceive has been established on the basis of assumptions.\n",
        "Life is just an assumption and you should be the one who can hypothesize, transcend, believe, execute, halt and moreover live with personal satisfaction.\"\"\",\n",
        "    \"\"\"It’s funny how most of the humans loathe cockroaches. Well, it’s in human nature to develop some kinds of fear against creepiness and Entomophobia persists really hard in the mind.\n",
        "Despite the fact, cockroaches have always intrigued me. Not because I like to kill ’em all; but how we humans are pretty similar to cockroaches in terms of behaviours and opportunities we ought to seek.\n",
        "Humans, the opportunity-seeking being, lash out during high time. A small opportunity tend to spawn dozens of humans from nowhere. We crawl out from the hell’s kitchen of our own mind to snatch something that might be ours if we really tried hard. Yet, before these dark times, we incubate our mind for lesser known reasons. Procrastination and precrastination sway our mind lest we be helding our seemingly-unartistic thoughts.\n",
        "There’s fear of beings with higher skills and intelligent. So, we try to slither quickly to rule over the things we want/wanted; like cockroaches trying to snatch the food before the humans pulverize them.\n",
        "We try to come clean\n",
        "It’s funny how we humans think cockroaches as the creature of dirt and roughness. We despise them, yet we are hidden from the fact that cockroaches are pretty clean insects. As enotmologist say, cockroaches constantly clean (groom) themselves. They run their legs and antennae over their bodies, then clean those appendages with their mouthparts. Ever see a cat groom? Cockroaches make cats look like slobs. It might be they despise us in fact.\n",
        "So, we try to come clean as well. We act as if we are the only thing that can do something for own greater good. We try to act cool, fake a smile, fake our social activities and above all fake the social-online status just to be treated like a mighty little creature.\n",
        "And that’s the reason we try to show ourselves clean. Clean in a sense of own twisted reputations that illusions the real “self”.\n",
        "We are creepy\n",
        "Cockroaches give you creeps. But we humans are no different. We slither over the land of hidden likeness, infatuation, love and lust. These are similar words with different sentiments. In our life we struggle hard to find someone we can have connection mentally. But that’s a long journey — a very long and timeless road. Before finding someone we feel connected to, the other phases try to creep into the mind; seep through our lustful nature.\n",
        "I don’t know what cockroaches are thinking\n",
        "But I do know humans are more savage, and lesser being than cockroaches.\n",
        "The nuisance to the nature.\n",
        "The resentment among each other.\n",
        "The creatures that try to live in a reality powered by chaos illusions….\"\"\",\n",
        "    \"\"\"life is just like this very stage where we keep on moving back/forth. And most of the time, we feel much comfortable around the middle part. Or just staying at one point.\n",
        "Learning is a continuous process. It happens throughout our life. Right from the birth till our inevitable fate, death.\"\"\",\n",
        "    \"\"\"We are all no-brainers. You have someone in your chat list who can really inspire you to learn stuffs. (Dog knows what, s/he really cares about you, perhaps.) Asking about the things you find difficult to grasp can really clear your confusion on the topics.\n",
        "You expose your true self by asking questions.\n",
        "If that’s a drag, just don’t fret about it.\n",
        "Take a nap.\n",
        "Reboot your mind for a fresh start for the formal actions mentioned.\n",
        "And, don’t forget to attain seminars and meetups for self-improvement. Interaction with people might boost your confidence at some level. Yes, I know being an introvert is a let down. But, hey! At least we can give it a try.\n",
        "Still we learn nothing\"\"\",\n",
        "    \"\"\"As the human civilization soar high through the unknown passage of time, knowledge has been the essence in every life. Knowledge is the information and we human thrive for information; not to smooth-out our whole life, but to find pleasure and happiness.\n",
        "Human beings rise on the giant shoulder of the knowledge that has been passed down from generations, so that the future won’t live like dead. The very books, scrolls, papers, that has powered the information pool of today is the key that our source of knowledge is enriched and well lived.\n",
        "Gathering knowledge is good enough to make one’s life. However being “wise” is more than simply having knowledge. It is one thing to memorize the books and facts. However it is the wise thing to use those knowledge to drive the forces for betterment. Boxing up concept is good enough , but ability to think outside the box is greater.\n",
        "(as In the passage by Bertrand Russell) The very example of nuclear outbreak is enough to make us realize the boundaries where knowledge and wisdom get separated as two entities. The creation of nuclear energy is from knowledge. However had mankind been wise enough, the atomic bombing would not be alive in the history books we read today.\n",
        "The very core of every civilization may be the knowledge that powers it. However, it is the wisest words earned through experience of life and death that could have made humans a boon to nature.\n",
        "You may know how to propose a girl (that’s knowledge). However it is the wisdom to find “true love and happiness”. He who refines the knowledge can seek the wisdom that was not sought. A person that has learnt the way of life and time may have wisdom because he/she has stood up from the mistakes that had been committed in the life.\n",
        "“Knowledge is power, wisdom is light”, as I say.\n",
        "We humans have developed and advanced so much through the knowledge that has been passed down from generation to generation, we are more informative and knowledgeable today than the people were in the past. The sole reason is “tons” of information that we have today. Be it from Sir Isaac Newton, Leonardo Da Vinci, Aristotle to Sigmund Freud to that professor who teaches in your class.\n",
        "I don’t mean to wipe out their legacy, here\n",
        "Well, they are all knowledgeable, informative and can make you understand the concept of so-called world. You may have better information from the information-pool of tons of books and the internet of lies, there are few in million who are wise enough.\n",
        "People in the past were much WISER than that of today. And it is not the matter of simplicity that defines the wisdom, it is much complex that ought to be. Life, death, self-awareness, experience, feelings, sense of priority, comprehensiveness, are all that summed up to refine a centered-knowledge into the wiser information for wisdom. We may have progressed in science, technology, health,etc through the infinite knowledge-pool we have today but we do not know much about how to teach wisdom. In the course of time, the wiser words had been abandoned in favour of personal or community improvements. Those awesome people of the past who learnt the hard way through struggle , experiences were wise enough to give you “revival” potion for your life, for humanity and mankind.\n",
        "Just as a growing child senses the world through a mere physical form, gradually gains the hold of the world outside, it is the wisdom that teaches that child to act in the puppeted-world to have a self-esteem and to love others; world-centered knowledge he/she gains and gradually gains wisdom.\n",
        "that is wisdom, an evolved knowledge; gained selfishly or selflessly\"\"\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYhKfY4YBCNq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5b8886de-8ba3-421d-925d-5122c4141296"
      },
      "source": [
        "cleaned_text,tokenized_text = normalize_corpus(data)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11/11 [00:00<00:00, 76.09it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWoaR2xjS5bO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "cbd915a7-8820-47f0-def8-e43e3fa5664c"
      },
      "source": [
        "[d[:100] for d in cleaned_text ]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['who are you',\n",
              " 'you are a bastardasshole and bitch',\n",
              " 'where do you live',\n",
              " 'what are your goals',\n",
              " 'what doe life mean to you',\n",
              " 'i bet you know about the classic mario game and probably have played it you are on a quest moving fr',\n",
              " 'i am paradox and there exists a theoretical limit to human mind that any imagination is not unbound ',\n",
              " 'it funny how most of the human loathe cockroaches well it in human nature to develop some kind of fe',\n",
              " 'life is just like this very stage where we keep on moving backforth and most of the time we feel muc',\n",
              " 'we are all nobrainers you have someone in your chat list who can really inspire you to learn stuffs ',\n",
              " 'a the human civilization soar high through the unknown passage of time knowledge ha been the essence']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiEGOhGNTWta",
        "colab_type": "text"
      },
      "source": [
        "Here, we use Universal Sentence Encoder to featurize each text.\n",
        "This will create some type of representation of text in latent space.\n",
        "The length of each vector is 512.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyH_xXuuTYYQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "442a6b1d-4bdd-42b8-ea47-50441d863cb2"
      },
      "source": [
        "sentence_embeds = get_features(cleaned_text)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNXfLzO_S5mL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "f1e5af0e-0c3a-4d02-c060-c147643c4694"
      },
      "source": [
        "sentence_embeds"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.06367727, -0.02018026, -0.00866594, ...,  0.01672676,\n",
              "        -0.04984674,  0.02839352],\n",
              "       [ 0.0143586 , -0.02608253, -0.04220014, ...,  0.08081238,\n",
              "        -0.1200572 , -0.01188981],\n",
              "       [-0.0777311 , -0.01065409, -0.0302369 , ...,  0.03215369,\n",
              "        -0.02641433,  0.01580315],\n",
              "       ...,\n",
              "       [-0.03547352, -0.05057082, -0.01285956, ..., -0.05603344,\n",
              "        -0.01884778, -0.03735124],\n",
              "       [-0.00815318,  0.00378894, -0.0077464 , ..., -0.00123372,\n",
              "        -0.00134031,  0.04868059],\n",
              "       [ 0.04676769,  0.01005963,  0.01288459, ...,  0.04599803,\n",
              "         0.0128504 , -0.03637344]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fshtiwidTvjl",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**Define Similarity Metric**\n",
        "We use cosine similarity to find simiarity between two vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYjaRpkdTxax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cosine_similarity(v1, v2):\n",
        "    mag1 = np.linalg.norm(v1)\n",
        "    mag2 = np.linalg.norm(v2)\n",
        "    if (not mag1) or (not mag2):\n",
        "        return 0\n",
        "    return np.dot(v1, v2) / (mag1 * mag2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1326Cm6jT1Pl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_similiarity(text1, text2):\n",
        "    vec1 = get_features(text1)[0]\n",
        "    vec2 = get_features(text2)[0]\n",
        "    print(vec1.shape)\n",
        "    return cosine_similarity(vec1, vec2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWNPqMciT3Zr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "ddce26c4-d63d-45a5-8495-a7a281b1a314"
      },
      "source": [
        "test_similiarity('that cat eats catnip', 'that cat drinks')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(512,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8648598"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1Q0k33lT70c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "90864834-9b04-411c-8bad-c653c9c62eef"
      },
      "source": [
        "\n",
        "test_similiarity('he made that food', 'that food was made by him')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(512,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.90939426"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtEmaHeSUMb_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "d88a19c2-3e54-4059-fd07-4cf3dcdeb6b8"
      },
      "source": [
        "test_similiarity('i am going to egypt', 'that food was made by him')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(512,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.18903711"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rutrsb1aUcS1",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### **Note**\n",
        "As seen from this test, the semantic matching is low for texts that are out of context with each other."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsqYlTbaUreU",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### **Semantic Matching/Search**\n",
        "Use the data we defined earlier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OeG3Ia-Ue4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def semantic_search(query, data, vectors):\n",
        "    query,tokens = normalize_corpus(query)\n",
        "    print(\"Extracting features...\")\n",
        "    print(query)\n",
        "    query_vec = get_features(query)[0].ravel()\n",
        "    res = []\n",
        "    for i, d in enumerate(data):\n",
        "        qvec = vectors[i].ravel()\n",
        "        sim = cosine_similarity(query_vec, qvec)\n",
        "        res.append((sim, d[:100], i))\n",
        "    return sorted(res, key=lambda x : x[0], reverse=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgb5h5iMU1Bb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "7a3197e4-d4e5-4992-acd6-b3cc6496fa66"
      },
      "source": [
        "semantic_search([\"motherfucker\"], cleaned_text, sentence_embeds)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 580.21it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting features...\n",
            "['motherfucker']\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.6115417, 'you are a bastardasshole and bitch', 1),\n",
              " (0.28766087, 'who are you', 0),\n",
              " (0.15605722, 'what doe life mean to you', 4),\n",
              " (0.15260382, 'where do you live', 2),\n",
              " (0.10140095,\n",
              "  'it funny how most of the human loathe cockroaches well it in human nature to develop some kind of fe',\n",
              "  7),\n",
              " (0.090148285,\n",
              "  'i am paradox and there exists a theoretical limit to human mind that any imagination is not unbound ',\n",
              "  6),\n",
              " (0.0771931,\n",
              "  'a the human civilization soar high through the unknown passage of time knowledge ha been the essence',\n",
              "  10),\n",
              " (0.059830137,\n",
              "  'i bet you know about the classic mario game and probably have played it you are on a quest moving fr',\n",
              "  5),\n",
              " (0.0432938,\n",
              "  'life is just like this very stage where we keep on moving backforth and most of the time we feel muc',\n",
              "  8),\n",
              " (0.034860376, 'what are your goals', 3),\n",
              " (-0.057085954,\n",
              "  'we are all nobrainers you have someone in your chat list who can really inspire you to learn stuffs ',\n",
              "  9)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_j6F7Io3S5g0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}